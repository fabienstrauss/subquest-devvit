{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NME8UxlnxWo8"
      },
      "source": [
        "# Interactive Story Generator - Complete Workflow\n",
        "\n",
        "This notebook creates interactive stories for SubQuest using a combination of local and external LLMs for optimal results.\n",
        "\n",
        "## Complete Workflow:\n",
        "1. **Setup Ollama** with Llama 3.1:8B for story concept refinement\n",
        "2. **Connect Google Drive** for automatic file storage (optional)\n",
        "3. **Configure Story Parameters** (nodes, choices, themes, atmosphere)\n",
        "4. **Refine Story Concept** using local Llama 3.1:8B\n",
        "5. **Generate External LLM Prompt** optimized for ChatGPT/Claude/Gemini\n",
        "6. **Create Story Structure** via external LLM (recommended) or local generation\n",
        "7. **Visualize Story Tree** for review and approval\n",
        "8. **Generate Context-Aware Image Prompts** using story analysis\n",
        "9. **Create Images** with memory-optimized Stable Diffusion\n",
        "10. **Export Final JSON** in SubQuest-compatible format\n",
        "\n",
        "**üí° Recommended Approach:** Use local Llama for concept refinement and prompt generation, then external LLMs (ChatGPT-4, Claude, Gemini) for actual story creation due to their superior context handling and consistency.\n",
        "\n",
        "**Run each cell in order for the complete experience!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-kM69IcxWo-"
      },
      "source": [
        "## üì¶ Step 1: Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42gIY4HoxWo-",
        "outputId": "7b70043e-16b9-4ffe-f9b3-af98942f71fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Running in Google Colab\n"
          ]
        }
      ],
      "source": [
        "# Core libraries\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import uuid\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "import networkx as nx\n",
        "\n",
        "# Google Colab integration\n",
        "try:\n",
        "    from google.colab import drive, files\n",
        "    IN_COLAB = True\n",
        "    print(\"üîó Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"üíª Running locally\")\n",
        "\n",
        "# Image generation (will install if needed)\n",
        "try:\n",
        "    from PIL import Image\n",
        "    import torch\n",
        "    from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
        "    IMAGES_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IMAGES_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è Image generation libraries not available - will install when needed\")\n",
        "\n",
        "print(\"‚úÖ Core packages imported successfully!\")\n",
        "print(\"üöÄ Ready to create interactive stories!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3NEtWplxWo_"
      },
      "source": [
        "## üîß Step 2: Install and Setup Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chvZuQE0xWpA"
      },
      "outputs": [],
      "source": [
        "class OllamaSetup:\n",
        "    def __init__(self):\n",
        "        self.base_url = \"http://localhost:11434\"\n",
        "        self.server_process = None\n",
        "\n",
        "    def install_ollama(self):\n",
        "        \"\"\"Install Ollama with GPU support.\"\"\"\n",
        "        print(\"üîß Installing Ollama...\")\n",
        "\n",
        "        try:\n",
        "            # Download and install Ollama\n",
        "            result = subprocess.run(\n",
        "                [\"curl\", \"-fsSL\", \"https://ollama.com/install.sh\"],\n",
        "                capture_output=True, text=True, check=True\n",
        "            )\n",
        "\n",
        "            install_result = subprocess.run(\n",
        "                [\"sh\"], input=result.stdout,\n",
        "                capture_output=True, text=True, check=True\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ Ollama installed successfully!\")\n",
        "\n",
        "            # Enable GPU support\n",
        "            os.environ['OLLAMA_USE_CUDA'] = '1'\n",
        "            print(\"üöÄ GPU acceleration enabled\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Installation failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def start_server(self):\n",
        "        \"\"\"Start Ollama server.\"\"\"\n",
        "        print(\"üîÑ Starting Ollama server...\")\n",
        "\n",
        "        try:\n",
        "            # Start server in background\n",
        "            self.server_process = subprocess.Popen(\n",
        "                [\"ollama\", \"serve\"],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                env=dict(os.environ, OLLAMA_USE_CUDA='1')\n",
        "            )\n",
        "\n",
        "            # Wait for server to start\n",
        "            time.sleep(5)\n",
        "\n",
        "            # Test connection\n",
        "            response = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                print(\"‚úÖ Ollama server is running!\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"‚ùå Server not responding: {response.status_code}\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to start server: {e}\")\n",
        "            return False\n",
        "\n",
        "    def check_gpu(self):\n",
        "        \"\"\"Check GPU availability.\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"],\n",
        "                capture_output=True, text=True, check=True\n",
        "            )\n",
        "\n",
        "            if result.stdout.strip():\n",
        "                gpu_info = result.stdout.strip().split(', ')\n",
        "                print(f\"üéÆ GPU: {gpu_info[0]}\")\n",
        "                print(f\"üíæ Memory: {gpu_info[1]}\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è No GPU detected - will use CPU\")\n",
        "                return False\n",
        "\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è GPU check failed - will use CPU\")\n",
        "            return False\n",
        "\n",
        "# Initialize Ollama setup\n",
        "ollama_setup = OllamaSetup()\n",
        "\n",
        "# Check GPU first\n",
        "has_gpu = ollama_setup.check_gpu()\n",
        "\n",
        "# Install and start Ollama\n",
        "if ollama_setup.install_ollama():\n",
        "    if ollama_setup.start_server():\n",
        "        print(\"\\nüéâ Ollama is ready for model installation!\")\n",
        "        if has_gpu:\n",
        "            print(\"üí° GPU detected - you can use larger models for better quality\")\n",
        "        else:\n",
        "            print(\"üí° No GPU - recommend using smaller models (1B-3B)\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Server failed to start\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Installation failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG6g-uQ7xWpA"
      },
      "source": [
        "## ü§ñ Step 3: Install Llama Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeNYEM6xxWpA"
      },
      "outputs": [],
      "source": [
        "class ModelManager:\n",
        "    def __init__(self):\n",
        "        self.base_url = \"http://localhost:11434\"\n",
        "        self.current_model = \"llama3.1:8b\"\n",
        "        self.required_model = \"llama3.1:8b\"\n",
        "\n",
        "    def setup_required_model(self):\n",
        "        \"\"\"Setup the required Llama 3.1:8B model.\"\"\"\n",
        "        print(\"ü§ñ Setting up Llama 3.1:8B Model\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"üìã This notebook requires Llama 3.1:8B for optimal story concept refinement.\")\n",
        "        print(\"üí° For actual story generation, we'll create a prompt for external LLMs.\")\n",
        "        print()\n",
        "\n",
        "        # Check if model is already available\n",
        "        downloaded_models = self.list_downloaded_models()\n",
        "\n",
        "        if self.required_model in downloaded_models:\n",
        "            print(f\"‚úÖ {self.required_model} is already available!\")\n",
        "            if self.test_model(self.required_model):\n",
        "                print(\"üéâ Model is ready for use!\")\n",
        "                return True\n",
        "\n",
        "        print(f\"üì• Installing {self.required_model}...\")\n",
        "        print(\"üìä Size: ~5.0GB - This may take several minutes\")\n",
        "        print(\"üí° This is a one-time download\")\n",
        "\n",
        "        if self.pull_model(self.required_model):\n",
        "            if self.test_model(self.required_model):\n",
        "                print(f\"üéâ {self.required_model} is ready!\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"‚ùå Model installed but not working properly\")\n",
        "                return False\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to install {self.required_model}\")\n",
        "            return False\n",
        "\n",
        "    def pull_model(self, model_name: str) -> bool:\n",
        "        \"\"\"Download a model from Ollama registry.\"\"\"\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{self.base_url}/api/pull\",\n",
        "                json={\"name\": model_name},\n",
        "                stream=True,\n",
        "                timeout=600  # 10 minute timeout for 8B model\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                print(\"üì¶ Downloading...\")\n",
        "                for line in response.iter_lines():\n",
        "                    if line:\n",
        "                        try:\n",
        "                            data = json.loads(line.decode('utf-8'))\n",
        "                            if 'status' in data:\n",
        "                                print(f\"\\r{data['status']}\", end='', flush=True)\n",
        "                            if data.get('status') == 'success':\n",
        "                                print(f\"\\n‚úÖ {model_name} downloaded successfully!\")\n",
        "                                return True\n",
        "                        except json.JSONDecodeError:\n",
        "                            continue\n",
        "\n",
        "                print(f\"\\n‚úÖ {model_name} download completed!\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"\\n‚ùå Failed to download {model_name}: HTTP {response.status_code}\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Download error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def test_model(self, model_name: str) -> bool:\n",
        "        \"\"\"Test if model is working.\"\"\"\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{self.base_url}/api/generate\",\n",
        "                json={\n",
        "                    \"model\": model_name,\n",
        "                    \"prompt\": \"Write one sentence about storytelling.\",\n",
        "                    \"stream\": False\n",
        "                },\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                text = result.get('response', '').strip()\n",
        "                if text:\n",
        "                    print(f\"‚úÖ Model test successful!\")\n",
        "                    return True\n",
        "            return False\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def list_downloaded_models(self) -> list:\n",
        "        \"\"\"Get list of downloaded models.\"\"\"\n",
        "        try:\n",
        "            response = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                return [model['name'] for model in data.get('models', [])]\n",
        "            return []\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "# Initialize and setup the required model\n",
        "model_manager = ModelManager()\n",
        "\n",
        "if model_manager.setup_required_model():\n",
        "    print(f\"\\nüéâ Ready to proceed with {model_manager.current_model}!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Could not setup required model. Please check your Ollama installation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkjO9EqdxWpA"
      },
      "source": [
        "## üíæ Step 4: Setup Google Drive Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuUCFdWpxWpA"
      },
      "outputs": [],
      "source": [
        "class DriveManager:\n",
        "    def __init__(self):\n",
        "        self.drive_mounted = False\n",
        "        self.drive_path = \"/content/drive\" if IN_COLAB else \"./local_drive\"\n",
        "        self.project_folder = None\n",
        "        self.use_drive = False\n",
        "\n",
        "    def setup_storage(self):\n",
        "        \"\"\"Interactive setup for storage options.\"\"\"\n",
        "        print(\"üíæ Storage Setup\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        if IN_COLAB:\n",
        "            choice = input(\"Connect Google Drive for automatic storage? (y/n): \").strip().lower()\n",
        "\n",
        "            if choice == 'y':\n",
        "                return self.connect_drive()\n",
        "            else:\n",
        "                print(\"üìÅ Using local storage (files will be downloadable)\")\n",
        "                self.use_drive = False\n",
        "                return self.setup_local_storage()\n",
        "        else:\n",
        "            print(\"üìÅ Running locally - using local storage\")\n",
        "            self.use_drive = False\n",
        "            return self.setup_local_storage()\n",
        "\n",
        "    def connect_drive(self):\n",
        "        \"\"\"Connect to Google Drive.\"\"\"\n",
        "        print(\"üîó Connecting to Google Drive...\")\n",
        "\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            self.drive_mounted = True\n",
        "            self.use_drive = True\n",
        "\n",
        "            # Create project folder\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            self.project_folder = f\"/content/drive/MyDrive/SubQuest_Stories/Story_{timestamp}\"\n",
        "\n",
        "            os.makedirs(self.project_folder, exist_ok=True)\n",
        "            os.makedirs(f\"{self.project_folder}/images\", exist_ok=True)\n",
        "\n",
        "            print(f\"‚úÖ Google Drive connected!\")\n",
        "            print(f\"üìÅ Project folder: {self.project_folder}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Drive connection failed: {e}\")\n",
        "            print(\"üìÅ Falling back to local storage\")\n",
        "            return self.setup_local_storage()\n",
        "\n",
        "    def setup_local_storage(self):\n",
        "        \"\"\"Setup local storage.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.project_folder = f\"./SubQuest_Story_{timestamp}\"\n",
        "\n",
        "        os.makedirs(self.project_folder, exist_ok=True)\n",
        "        os.makedirs(f\"{self.project_folder}/images\", exist_ok=True)\n",
        "\n",
        "        print(f\"‚úÖ Local storage ready!\")\n",
        "        print(f\"üìÅ Project folder: {self.project_folder}\")\n",
        "        return True\n",
        "\n",
        "    def get_image_base_url(self):\n",
        "        \"\"\"Get base URL for images.\"\"\"\n",
        "        if self.use_drive:\n",
        "            # For Drive, we'll use direct file IDs later\n",
        "            return \"drive://\"\n",
        "        else:\n",
        "            # Ask user for base URL\n",
        "            print(\"\\nüåê Image URL Configuration\")\n",
        "            print(\"For local storage, you need to specify where images will be hosted.\")\n",
        "\n",
        "            base_url = input(\"Enter base URL for images (e.g., 'https://mysite.com/images/'): \").strip()\n",
        "\n",
        "            if not base_url:\n",
        "                base_url = \"./images/\"  # Relative path default\n",
        "                print(f\"Using default: {base_url}\")\n",
        "\n",
        "            return base_url\n",
        "\n",
        "# Initialize drive manager\n",
        "drive_manager = DriveManager()\n",
        "\n",
        "# Interactive storage setup\n",
        "if drive_manager.setup_storage():\n",
        "    image_base_url = drive_manager.get_image_base_url()\n",
        "    print(f\"\\nüéâ Storage configured successfully!\")\n",
        "    print(f\"üìÅ Project: {drive_manager.project_folder}\")\n",
        "    print(f\"üñºÔ∏è Image base URL: {image_base_url}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Storage setup failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ltHud6LxWpB"
      },
      "source": [
        "## ‚öôÔ∏è Step 5: Configure Story Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6aOEKKYxWpB"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class StoryConfig:\n",
        "    \"\"\"Complete story configuration.\"\"\"\n",
        "    title: str\n",
        "    description: str\n",
        "    theme: str\n",
        "    target_nodes: int\n",
        "    max_choices_per_node: int\n",
        "    min_choices_per_node: int\n",
        "    allow_fan_in: bool  # Multiple paths to same node\n",
        "    ending_nodes: int\n",
        "    story_concept: str\n",
        "    tone: str\n",
        "    target_audience: str\n",
        "\n",
        "class StoryConfigurator:\n",
        "    def __init__(self):\n",
        "        self.config = None\n",
        "\n",
        "    def interactive_setup(self):\n",
        "        \"\"\"Interactive story configuration.\"\"\"\n",
        "        print(\"‚öôÔ∏è Story Configuration\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"Let's configure your interactive story parameters!\")\n",
        "        print()\n",
        "\n",
        "        # Basic story info\n",
        "        title = input(\"üìñ Story Title: \").strip()\n",
        "        if not title:\n",
        "            title = \"My Interactive Adventure\"\n",
        "\n",
        "        description = input(\"üìù Brief Description: \").strip()\n",
        "        if not description:\n",
        "            description = \"An exciting interactive story\"\n",
        "\n",
        "        theme = input(\"üé≠ Theme/Genre (fantasy, sci-fi, mystery, adventure, etc.): \").strip()\n",
        "        if not theme:\n",
        "            theme = \"adventure\"\n",
        "\n",
        "        # Story structure\n",
        "        print(\"\\nüìä Story Structure Configuration:\")\n",
        "\n",
        "        target_nodes = self._get_int_input(\n",
        "            \"üéØ Target number of story nodes (5-15 recommended): \",\n",
        "            default=8, min_val=3, max_val=20\n",
        "        )\n",
        "\n",
        "        max_choices = self._get_int_input(\n",
        "            \"üîÄ Maximum choices per node (2-4 recommended): \",\n",
        "            default=3, min_val=2, max_val=5\n",
        "        )\n",
        "\n",
        "        min_choices = self._get_int_input(\n",
        "            \"üîÄ Minimum choices per node (2-3 recommended): \",\n",
        "            default=2, min_val=2, max_val=max_choices\n",
        "        )\n",
        "\n",
        "        # Advanced options\n",
        "        print(\"\\nüîß Advanced Options:\")\n",
        "\n",
        "        fan_in_choice = input(\"üîÑ Allow fan-in (multiple paths to same node)? (y/n): \").strip().lower()\n",
        "        allow_fan_in = fan_in_choice == 'y'\n",
        "\n",
        "        ending_nodes = self._get_int_input(\n",
        "            \"üèÅ Number of different endings (2-5 recommended): \",\n",
        "            default=3, min_val=1, max_val=8\n",
        "        )\n",
        "\n",
        "        # Story details\n",
        "        print(\"\\n‚úçÔ∏è Story Details:\")\n",
        "\n",
        "        story_concept = input(\"üí° Story Concept (describe your story idea): \").strip()\n",
        "        if not story_concept:\n",
        "            story_concept = f\"An interactive {theme} story with meaningful choices\"\n",
        "\n",
        "        tone = input(\"üé® Tone (serious, humorous, dark, light, etc.): \").strip()\n",
        "        if not tone:\n",
        "            tone = \"engaging\"\n",
        "\n",
        "        audience = input(\"üë• Target Audience (all ages, teen, adult, etc.): \").strip()\n",
        "        if not audience:\n",
        "            audience = \"all ages\"\n",
        "\n",
        "        # Create configuration\n",
        "        self.config = StoryConfig(\n",
        "            title=title,\n",
        "            description=description,\n",
        "            theme=theme,\n",
        "            target_nodes=target_nodes,\n",
        "            max_choices_per_node=max_choices,\n",
        "            min_choices_per_node=min_choices,\n",
        "            allow_fan_in=allow_fan_in,\n",
        "            ending_nodes=ending_nodes,\n",
        "            story_concept=story_concept,\n",
        "            tone=tone,\n",
        "            target_audience=audience\n",
        "        )\n",
        "\n",
        "        self.display_config()\n",
        "        return self.config\n",
        "\n",
        "    def _get_int_input(self, prompt: str, default: int, min_val: int, max_val: int) -> int:\n",
        "        \"\"\"Get integer input with validation.\"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                value = input(f\"{prompt}(default: {default}): \").strip()\n",
        "                if not value:\n",
        "                    return default\n",
        "\n",
        "                num = int(value)\n",
        "                if min_val <= num <= max_val:\n",
        "                    return num\n",
        "                else:\n",
        "                    print(f\"‚ùå Please enter a number between {min_val} and {max_val}\")\n",
        "\n",
        "            except ValueError:\n",
        "                print(\"‚ùå Please enter a valid number\")\n",
        "\n",
        "    def display_config(self):\n",
        "        \"\"\"Display current configuration.\"\"\"\n",
        "        if not self.config:\n",
        "            print(\"‚ùå No configuration available\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n‚úÖ Story Configuration Summary:\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"üìñ Title: {self.config.title}\")\n",
        "        print(f\"üìù Description: {self.config.description}\")\n",
        "        print(f\"üé≠ Theme: {self.config.theme}\")\n",
        "        print(f\"üéØ Target Nodes: {self.config.target_nodes}\")\n",
        "        print(f\"üîÄ Choices per Node: {self.config.min_choices_per_node}-{self.config.max_choices_per_node}\")\n",
        "        print(f\"üîÑ Fan-in Allowed: {'Yes' if self.config.allow_fan_in else 'No'}\")\n",
        "        print(f\"üèÅ Ending Nodes: {self.config.ending_nodes}\")\n",
        "        print(f\"üí° Concept: {self.config.story_concept}\")\n",
        "        print(f\"üé® Tone: {self.config.tone}\")\n",
        "        print(f\"üë• Audience: {self.config.target_audience}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "# Interactive configuration\n",
        "configurator = StoryConfigurator()\n",
        "story_config = configurator.interactive_setup()\n",
        "\n",
        "print(\"\\nüéâ Configuration complete! Ready for prompt generation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Step 6: Generate Optimized Prompt with LLM\n",
        "\n",
        "This step uses your local Llama 3.1:8B model to refine your story concept and create an optimized prompt for external LLMs. The local model analyzes your basic concept and expands it into a rich, detailed story description perfect for interactive storytelling.\n",
        "\n",
        "The generated prompt is specifically designed for ChatGPT, Claude, or Gemini, which have larger context windows and better consistency for complex story generation.\n"
      ],
      "metadata": {
        "id": "azo2xeYs_IqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExternalPromptGenerator:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        self.base_url = \"http://localhost:11434\"\n",
        "\n",
        "    def refine_story_concept(self, config) -> str:\n",
        "        \"\"\"Use local LLM to refine the story concept.\"\"\"\n",
        "        print(\"üß† Refining story concept with Llama 3.1:8B...\")\n",
        "\n",
        "        refinement_prompt = f\"\"\"You are a creative writing expert. Take this basic story concept and expand it into a rich, detailed story description perfect for interactive storytelling.\n",
        "\n",
        "Original Story Details:\n",
        "- Title: {config.title}\n",
        "- Theme: {config.theme}\n",
        "- Basic Concept: {config.story_concept}\n",
        "- Tone: {config.tone}\n",
        "- Target Audience: {config.target_audience}\n",
        "\n",
        "Technical Requirements:\n",
        "- Will have {config.target_nodes} story nodes\n",
        "- Each node will have {config.min_choices_per_node}-{config.max_choices_per_node} choices\n",
        "- Will have {config.ending_nodes} different endings\n",
        "- Fan-in allowed: {'Yes' if config.allow_fan_in else 'No'}\n",
        "\n",
        "Create an enhanced story concept that includes:\n",
        "1. A compelling opening scenario with specific setting details\n",
        "2. Key characters with clear motivations and backgrounds\n",
        "3. The central conflict or challenge driving the story\n",
        "4. Potential plot developments, twists, and complications\n",
        "5. How different choices lead to meaningful consequences\n",
        "6. Varied ending possibilities with different outcomes\n",
        "7. Rich atmospheric and world-building details\n",
        "8. Specific scenarios where player agency matters\n",
        "\n",
        "Make it detailed, engaging, and perfect for creating {config.target_nodes} interconnected story nodes. Focus on creating scenarios where choices have real impact on the story's direction and outcome.\n",
        "\n",
        "Return only the enhanced story concept, nothing else.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{self.base_url}/api/generate\",\n",
        "                json={\n",
        "                    \"model\": self.model_name,\n",
        "                    \"prompt\": refinement_prompt,\n",
        "                    \"stream\": False,\n",
        "                    \"options\": {\n",
        "                        \"temperature\": 0.8,\n",
        "                        \"top_p\": 0.9,\n",
        "                        \"num_ctx\": 4096\n",
        "                    }\n",
        "                },\n",
        "                timeout=120\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                refined_concept = result.get('response', '').strip()\n",
        "\n",
        "                if refined_concept:\n",
        "                    print(\"‚úÖ Story concept refined!\")\n",
        "                    return refined_concept\n",
        "                else:\n",
        "                    print(\"‚ùå Empty response, using original concept\")\n",
        "                    return config.story_concept\n",
        "            else:\n",
        "                print(f\"‚ùå Refinement failed: HTTP {response.status_code}\")\n",
        "                return config.story_concept\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error refining concept: {e}\")\n",
        "            return config.story_concept\n",
        "\n",
        "    def generate_external_prompt(self, config, refined_concept: str) -> str:\n",
        "        \"\"\"Generate a complete prompt for external LLMs like ChatGPT or Gemini.\"\"\"\n",
        "\n",
        "        prompt_text = f\"\"\"# Interactive Story Generation Task\n",
        "\n",
        "You are tasked with creating a complete interactive story in JSON format. Follow these specifications exactly:\n",
        "\n",
        "## Story Requirements\n",
        "\n",
        "**Basic Information:**\n",
        "- Title: {config.title}\n",
        "- Description: {config.description}\n",
        "- Theme: {config.theme}\n",
        "- Tone: {config.tone}\n",
        "- Target Audience: {config.target_audience}\n",
        "\n",
        "**Enhanced Story Concept:**\n",
        "{refined_concept}\n",
        "\n",
        "**Technical Specifications:**\n",
        "- Total nodes: {config.target_nodes}\n",
        "- Choices per non-ending node: {config.min_choices_per_node} to {config.max_choices_per_node}\n",
        "- Number of different endings: {config.ending_nodes}\n",
        "- Fan-in allowed: {'Yes' if config.allow_fan_in else 'No'}\n",
        "- All ending nodes must be reachable from the start node\n",
        "\n",
        "## Critical Requirements\n",
        "\n",
        "1. RETURN ONLY THE JSON - No explanations, no markdown formatting, no additional text\n",
        "2. Every ending node must be reachable through valid choice paths from the start\n",
        "3. Use descriptive node IDs (not just numbers like \"node1\", \"node2\")\n",
        "4. Each story node should have 2-3 engaging paragraphs of content\n",
        "5. Choices must have meaningful consequences that affect the story outcome\n",
        "\n",
        "## Exact JSON Format Required\n",
        "\n",
        "Return ONLY the JSON in this exact format:\n",
        "\n",
        "{{\n",
        "  \"title\": \"{config.title}\",\n",
        "  \"description\": \"{config.description}\",\n",
        "  \"startNodeId\": \"opening_scene\",\n",
        "  \"nodes\": {{\n",
        "    \"opening_scene\": {{\n",
        "      \"id\": \"opening_scene\",\n",
        "      \"title\": \"Descriptive Scene Title\",\n",
        "      \"content\": \"Engaging story content here. Write 2-3 paragraphs that immerse the reader in the scene. Include sensory details, character thoughts, and atmospheric elements that bring the story to life.\",\n",
        "      \"imageUrl\": \"placeholder.jpg\",\n",
        "      \"choices\": [\n",
        "        {{\n",
        "          \"id\": \"choice_1\",\n",
        "          \"text\": \"Descriptive choice that shows consequences\",\n",
        "          \"nextNodeId\": \"consequence_node_1\"\n",
        "        }},\n",
        "        {{\n",
        "          \"id\": \"choice_2\",\n",
        "          \"text\": \"Alternative choice with different outcome\",\n",
        "          \"nextNodeId\": \"consequence_node_2\"\n",
        "        }}\n",
        "      ]\n",
        "    }},\n",
        "    \"consequence_node_1\": {{\n",
        "      \"id\": \"consequence_node_1\",\n",
        "      \"title\": \"Result of First Choice\",\n",
        "      \"content\": \"Content showing the consequences of the first choice. Continue the narrative based on the player's decision.\",\n",
        "      \"imageUrl\": \"placeholder.jpg\",\n",
        "      \"choices\": [\n",
        "        {{\n",
        "          \"id\": \"choice_3\",\n",
        "          \"text\": \"Next decision point\",\n",
        "          \"nextNodeId\": \"ending_1\"\n",
        "        }}\n",
        "      ]\n",
        "    }},\n",
        "    \"ending_1\": {{\n",
        "      \"id\": \"ending_1\",\n",
        "      \"title\": \"One Possible Ending\",\n",
        "      \"content\": \"A complete ending that resolves the story based on the choices made. Make it satisfying and conclusive.\",\n",
        "      \"imageUrl\": \"placeholder.jpg\",\n",
        "      \"isEnd\": true\n",
        "    }}\n",
        "  }}\n",
        "}}\n",
        "\n",
        "## Validation Rules\n",
        "\n",
        "- All nextNodeId values must reference actual node IDs in the nodes object\n",
        "- Ending nodes must have \"isEnd\": true and NO choices array\n",
        "- Non-ending nodes must NOT have the isEnd field\n",
        "- Every node must have: id, title, content, imageUrl\n",
        "- Every choice must have: id, text, nextNodeId\n",
        "- Ensure exactly {config.target_nodes} total nodes\n",
        "- Ensure exactly {config.ending_nodes} ending nodes\n",
        "- Make sure all endings are reachable through different choice paths\n",
        "\n",
        "## Content Guidelines\n",
        "\n",
        "- Write engaging, immersive content for each node\n",
        "- Make choices meaningful - they should lead to genuinely different outcomes\n",
        "- Include rich descriptions that help visualize the scenes\n",
        "- Maintain consistency with the {config.theme} theme and {config.tone} tone\n",
        "- Keep the {config.target_audience} audience in mind\n",
        "- Each node should advance the story or develop character/plot\n",
        "\n",
        "Generate the complete interactive story now. Remember: RETURN ONLY THE JSON, nothing else.\"\"\"\n",
        "\n",
        "        return prompt_text\n",
        "\n",
        "    def display_external_prompt(self, prompt: str):\n",
        "        \"\"\"Display the prompt for copying to external LLMs.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üöÄ EXTERNAL LLM PROMPT READY\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"üìã Copy the prompt below and paste it into ChatGPT, Claude, or Gemini:\")\n",
        "        print(\"üí° These models have larger context windows and can handle complex stories better.\")\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(prompt)\n",
        "        print(\"-\"*80)\n",
        "        print(\"\\nüìù Instructions:\")\n",
        "        print(\"1. Copy the entire prompt above\")\n",
        "        print(\"2. Paste it into your preferred LLM (ChatGPT-4, Claude, Gemini Pro)\")\n",
        "        print(\"3. Wait for the JSON response\")\n",
        "        print(\"4. Copy the JSON output\")\n",
        "        print(\"5. Return to this notebook and paste it in the next step\")\n",
        "        print(\"\\n‚ö†Ô∏è Important: Make sure you get ONLY the JSON in the response!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    def review_and_approve_concept(self, original_concept: str, refined_concept: str) -> str:\n",
        "        \"\"\"Allow user to review and modify the refined concept.\"\"\"\n",
        "        print(\"\\nüìã Story Concept Refinement Review\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"ORIGINAL CONCEPT:\")\n",
        "        print(\"-\" * 30)\n",
        "        print(original_concept)\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"REFINED CONCEPT:\")\n",
        "        print(\"-\" * 30)\n",
        "        print(refined_concept)\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        while True:\n",
        "            choice = input(\"\\nApprove refined concept? (y)es / (e)dit / (o)riginal / (r)egenerate: \").strip().lower()\n",
        "\n",
        "            if choice in ['y', 'yes']:\n",
        "                print(\"‚úÖ Refined concept approved!\")\n",
        "                return refined_concept\n",
        "            elif choice in ['e', 'edit']:\n",
        "                print(\"\\n‚úèÔ∏è Enter your custom concept:\")\n",
        "                custom_concept = input(\"> \")\n",
        "                if custom_concept.strip():\n",
        "                    print(\"‚úÖ Custom concept set!\")\n",
        "                    return custom_concept\n",
        "                else:\n",
        "                    print(\"‚ùå Empty concept, keeping refined version\")\n",
        "                    return refined_concept\n",
        "            elif choice in ['o', 'original']:\n",
        "                print(\"‚úÖ Using original concept\")\n",
        "                return original_concept\n",
        "            elif choice in ['r', 'regenerate']:\n",
        "                print(\"üîÑ Regenerating concept...\")\n",
        "                return self.refine_story_concept(story_config)\n",
        "            else:\n",
        "                print(\"‚ùå Invalid choice. Please enter 'y', 'e', 'o', or 'r'\")\n",
        "\n",
        "# Generate refined concept and external prompt\n",
        "prompt_generator = ExternalPromptGenerator(model_manager.current_model)\n",
        "\n",
        "# Step 1: Refine the story concept\n",
        "print(\"üéØ Step 1: Refining your story concept...\")\n",
        "refined_concept = prompt_generator.refine_story_concept(story_config)\n",
        "\n",
        "# Step 2: Review and approve the concept\n",
        "final_story_concept = prompt_generator.review_and_approve_concept(\n",
        "    story_config.story_concept,\n",
        "    refined_concept\n",
        ")\n",
        "\n",
        "# Step 3: Generate external prompt\n",
        "print(\"\\nüéØ Step 2: Generating external LLM prompt...\")\n",
        "external_prompt = prompt_generator.generate_external_prompt(story_config, final_story_concept)\n",
        "\n",
        "# Step 4: Display the prompt\n",
        "prompt_generator.display_external_prompt(external_prompt)\n",
        "\n",
        "print(\"\\n‚úÖ External prompt generated! Use it with ChatGPT, Claude, or Gemini for best results.\")\n",
        "print(\"üöÄ Ready to proceed to Step 7 for story generation!\")\n"
      ],
      "metadata": {
        "id": "LdMGWvaa_KMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Step 7: Generate Story Structure\n",
        "\n",
        "Choose between two generation methods:\n",
        "\n",
        "1. **Local Generation (Llama 3.1:8B)**: Fully automated but limited by context window\n",
        "   - Best for: Simple stories (5-8 nodes)\n",
        "   - Pros: No manual steps, immediate results\n",
        "   - Cons: May struggle with complex narratives or consistency\n",
        "\n",
        "2. **External LLM Generation (RECOMMENDED)**: Manual copy-paste but superior quality\n",
        "   - Best for: Complex stories (8+ nodes), better narrative consistency\n",
        "   - Pros: Larger context windows, better story coherence, handles complex branching\n",
        "   - Cons: Requires manual copy-paste step\n",
        "\n",
        "**üí° Recommendation:** Use external LLM generation for best results, especially for\n",
        "stories with multiple endings or complex choice consequences.\n",
        "\n"
      ],
      "metadata": {
        "id": "z2KFjupQ_2z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if Ollama server is running and restart if needed\n",
        "def check_and_restart_ollama():\n",
        "    \"\"\"Check if Ollama is running and restart if needed.\"\"\"\n",
        "    print(\"üîç Checking Ollama server status...\")\n",
        "\n",
        "    try:\n",
        "        # Test connection\n",
        "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Ollama server is running!\")\n",
        "            return True\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print(\"‚ùå Ollama server not running. Starting it...\")\n",
        "\n",
        "    try:\n",
        "        # Start Ollama server in background\n",
        "        import subprocess\n",
        "        import time\n",
        "\n",
        "        # Kill any existing Ollama processes\n",
        "        subprocess.run([\"pkill\", \"-f\", \"ollama\"], capture_output=True)\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Start new server\n",
        "        server_process = subprocess.Popen(\n",
        "            [\"ollama\", \"serve\"],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            env=dict(os.environ, OLLAMA_USE_CUDA='1')\n",
        "        )\n",
        "\n",
        "        # Wait for server to start\n",
        "        print(\"‚è≥ Starting Ollama server...\")\n",
        "        time.sleep(10)\n",
        "\n",
        "        # Test connection again\n",
        "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Ollama server restarted successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå Server started but not responding properly\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to start Ollama server: {e}\")\n",
        "        return False\n",
        "\n",
        "# Check server before generating story\n",
        "if check_and_restart_ollama():\n",
        "    print(\"üöÄ Ready to generate story!\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot start Ollama server. Please check your installation.\")\n",
        "\n",
        "@dataclass\n",
        "class Choice:\n",
        "    \"\"\"Represents a choice in the story.\"\"\"\n",
        "    id: str\n",
        "    text: str\n",
        "    nextNodeId: str\n",
        "\n",
        "@dataclass\n",
        "class StoryNode:\n",
        "    \"\"\"Represents a node in the story graph.\"\"\"\n",
        "    id: str\n",
        "    title: str\n",
        "    content: str\n",
        "    imageUrl: str\n",
        "    choices: List[Choice] = field(default_factory=list)\n",
        "    isEnd: bool = False\n",
        "\n",
        "@dataclass\n",
        "class Story:\n",
        "    \"\"\"Represents the complete story structure.\"\"\"\n",
        "    title: str\n",
        "    description: str\n",
        "    startNodeId: str\n",
        "    nodes: Dict[str, StoryNode] = field(default_factory=dict)\n",
        "\n",
        "class StoryGenerationManager:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        self.base_url = \"http://localhost:11434\"\n",
        "\n",
        "    def choose_generation_method(self, config, refined_concept: str):\n",
        "        \"\"\"Let user choose between internal or external generation.\"\"\"\n",
        "        print(\"üéØ Story Generation Options\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"Choose how you want to generate your story:\")\n",
        "        print()\n",
        "        print(\"1. üì± Generate in Notebook (Llama 3.1:8B)\")\n",
        "        print(\"   ‚úÖ Fully automated workflow\")\n",
        "        print(\"   ‚ö†Ô∏è Limited by local model context window\")\n",
        "        print(\"   üí° Best for smaller stories (5-8 nodes)\")\n",
        "        print()\n",
        "        print(\"2. üåê Generate with External LLM (ChatGPT/Claude/Gemini)\")\n",
        "        print(\"   ‚úÖ Larger context windows, better quality\")\n",
        "        print(\"   ‚úÖ Can handle complex stories (10+ nodes)\")\n",
        "        print(\"   ‚ö†Ô∏è Requires manual copy-paste step\")\n",
        "        print()\n",
        "\n",
        "        while True:\n",
        "            choice = input(\"Choose generation method (1 or 2): \").strip()\n",
        "\n",
        "            if choice == \"1\":\n",
        "                print(\"üöÄ Using internal generation with Llama 3.1:8B\")\n",
        "                return self.generate_internal_story(config, refined_concept)\n",
        "            elif choice == \"2\":\n",
        "                print(\"üåê Preparing external LLM prompt\")\n",
        "                return self.setup_external_generation(config, refined_concept)\n",
        "            else:\n",
        "                print(\"‚ùå Please enter 1 or 2\")\n",
        "\n",
        "    def generate_internal_story(self, config, refined_concept: str):\n",
        "        \"\"\"Generate story using local Llama model.\"\"\"\n",
        "        print(\"\\nüìö Generating story with local Llama 3.1:8B...\")\n",
        "\n",
        "        # Create internal prompt (simpler for local model)\n",
        "        prompt = self.create_internal_prompt(config, refined_concept)\n",
        "\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            if attempt > 0:\n",
        "                print(f\"üîÑ Attempt {attempt + 1}/{max_attempts}...\")\n",
        "\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    f\"{self.base_url}/api/generate\",\n",
        "                    json={\n",
        "                        \"model\": self.model_name,\n",
        "                        \"prompt\": prompt,\n",
        "                        \"stream\": False,\n",
        "                        \"options\": {\n",
        "                            \"temperature\": 0.3,\n",
        "                            \"top_p\": 0.8,\n",
        "                            \"num_ctx\": 8192,\n",
        "                            \"num_predict\": 6000\n",
        "                        }\n",
        "                    },\n",
        "                    timeout=300\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    story_text = result.get('response', '').strip()\n",
        "\n",
        "                    if story_text:\n",
        "                        print(\"‚úÖ Story generated! Parsing JSON...\")\n",
        "                        story = self.parse_story_json(story_text)\n",
        "                        if story:\n",
        "                            return story\n",
        "                        else:\n",
        "                            print(f\"‚ùå Attempt {attempt + 1} failed, trying again...\")\n",
        "                            continue\n",
        "                    else:\n",
        "                        print(\"‚ùå Empty response from LLM\")\n",
        "                        continue\n",
        "                else:\n",
        "                    print(f\"‚ùå Generation failed: HTTP {response.status_code}\")\n",
        "                    continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"‚ùå All {max_attempts} attempts failed\")\n",
        "        print(\"üí° Try using external LLM generation for better results\")\n",
        "        return None\n",
        "\n",
        "    def create_internal_prompt(self, config, refined_concept: str) -> str:\n",
        "        \"\"\"Create a simpler prompt optimized for local Llama model.\"\"\"\n",
        "        return f\"\"\"Create an interactive {config.theme} story in JSON format.\n",
        "\n",
        "Story: {config.title}\n",
        "Theme: {config.theme}\n",
        "Concept: {refined_concept}\n",
        "\n",
        "Requirements:\n",
        "- {config.target_nodes} total nodes\n",
        "- {config.ending_nodes} ending nodes\n",
        "- {config.min_choices_per_node}-{config.max_choices_per_node} choices per node\n",
        "- All endings must be reachable\n",
        "\n",
        "Return ONLY this JSON structure:\n",
        "{{\n",
        "  \"title\": \"{config.title}\",\n",
        "  \"description\": \"{config.description}\",\n",
        "  \"startNodeId\": \"start\",\n",
        "  \"nodes\": {{\n",
        "    \"start\": {{\n",
        "      \"id\": \"start\",\n",
        "      \"title\": \"Opening Scene\",\n",
        "      \"content\": \"Story content here (2 paragraphs).\",\n",
        "      \"imageUrl\": \"placeholder.jpg\",\n",
        "      \"choices\": [\n",
        "        {{\"id\": \"choice1\", \"text\": \"First choice\", \"nextNodeId\": \"node2\"}},\n",
        "        {{\"id\": \"choice2\", \"text\": \"Second choice\", \"nextNodeId\": \"node3\"}}\n",
        "      ]\n",
        "    }},\n",
        "    \"node2\": {{\n",
        "      \"id\": \"node2\",\n",
        "      \"title\": \"Scene Title\",\n",
        "      \"content\": \"Content based on first choice.\",\n",
        "      \"imageUrl\": \"placeholder.jpg\",\n",
        "      \"choices\": [{{\"id\": \"choice3\", \"text\": \"Next choice\", \"nextNodeId\": \"ending1\"}}]\n",
        "    }},\n",
        "    \"ending1\": {{\n",
        "      \"id\": \"ending1\",\n",
        "      \"title\": \"Ending Title\",\n",
        "      \"content\": \"Ending content.\",\n",
        "      \"imageUrl\": \"placeholder.jpg\",\n",
        "      \"isEnd\": true\n",
        "    }}\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Generate the complete story JSON now:\"\"\"\n",
        "\n",
        "    def setup_external_generation(self, config, refined_concept: str):\n",
        "        \"\"\"Setup external LLM generation.\"\"\"\n",
        "        print(\"üåê Using the external prompt from Step 6...\")\n",
        "        print(\"üí° Copy the prompt from Step 6 and use it with ChatGPT/Claude/Gemini\")\n",
        "\n",
        "        # Wait for user to come back with JSON\n",
        "        return self.input_external_json()\n",
        "\n",
        "    def input_external_json(self):\n",
        "        \"\"\"Get JSON input from user.\"\"\"\n",
        "        print(\"\\nüì• Paste your JSON from the external LLM:\")\n",
        "        print(\"(Press Enter twice when done)\")\n",
        "\n",
        "        json_lines = []\n",
        "        empty_count = 0\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                line = input()\n",
        "                if line.strip() == \"\":\n",
        "                    empty_count += 1\n",
        "                    if empty_count >= 2:\n",
        "                        break\n",
        "                else:\n",
        "                    empty_count = 0\n",
        "                    json_lines.append(line)\n",
        "            except (EOFError, KeyboardInterrupt):\n",
        "                break\n",
        "\n",
        "        if not json_lines:\n",
        "            print(\"‚ùå No input received\")\n",
        "            return None\n",
        "\n",
        "        json_text = \"\\n\".join(json_lines)\n",
        "        return self.parse_story_json(json_text)\n",
        "\n",
        "    def parse_story_json(self, story_text: str):\n",
        "        \"\"\"Parse JSON and convert to Story object.\"\"\"\n",
        "        try:\n",
        "            # Clean the text\n",
        "            story_text = story_text.strip()\n",
        "\n",
        "            # Remove markdown if present\n",
        "            if \"```json\" in story_text:\n",
        "                story_text = story_text.split(\"```json\")[1].split(\"```\")[0]\n",
        "            elif \"```\" in story_text:\n",
        "                story_text = story_text.split(\"```\")[1].split(\"```\")[0]\n",
        "\n",
        "            # Find JSON boundaries\n",
        "            json_start = story_text.find('{')\n",
        "            json_end = story_text.rfind('}') + 1\n",
        "\n",
        "            if json_start == -1 or json_end == 0:\n",
        "                print(\"‚ùå No valid JSON found\")\n",
        "                return None\n",
        "\n",
        "            json_text = story_text[json_start:json_end]\n",
        "            story_data = json.loads(json_text)\n",
        "\n",
        "            # Validate and convert\n",
        "            if self.validate_story_data(story_data):\n",
        "                story = self.convert_to_story_object(story_data)\n",
        "                if story:\n",
        "                    print(\"‚úÖ Story loaded successfully!\")\n",
        "                    print(f\"üìä {len(story.nodes)} nodes, {len([n for n in story.nodes.values() if n.isEnd])} endings\")\n",
        "                    return story\n",
        "\n",
        "            return None\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå JSON error: {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Processing error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def validate_story_data(self, data: dict) -> bool:\n",
        "        \"\"\"Basic validation of story structure.\"\"\"\n",
        "        required = ['title', 'description', 'startNodeId', 'nodes']\n",
        "        for field in required:\n",
        "            if field not in data:\n",
        "                print(f\"‚ùå Missing field: {field}\")\n",
        "                return False\n",
        "\n",
        "        if data['startNodeId'] not in data['nodes']:\n",
        "            print(\"‚ùå Start node not found\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def convert_to_story_object(self, data: dict):\n",
        "        \"\"\"Convert JSON to Story object.\"\"\"\n",
        "        try:\n",
        "            story = Story(\n",
        "                title=data['title'],\n",
        "                description=data['description'],\n",
        "                startNodeId=data['startNodeId']\n",
        "            )\n",
        "\n",
        "            for node_id, node_data in data['nodes'].items():\n",
        "                choices = []\n",
        "                if 'choices' in node_data:\n",
        "                    for choice_data in node_data['choices']:\n",
        "                        choices.append(Choice(\n",
        "                            id=choice_data['id'],\n",
        "                            text=choice_data['text'],\n",
        "                            nextNodeId=choice_data['nextNodeId']\n",
        "                        ))\n",
        "\n",
        "                node = StoryNode(\n",
        "                    id=node_data['id'],\n",
        "                    title=node_data['title'],\n",
        "                    content=node_data['content'],\n",
        "                    imageUrl=node_data.get('imageUrl', 'placeholder.jpg'),\n",
        "                    choices=choices,\n",
        "                    isEnd=node_data.get('isEnd', False)\n",
        "                )\n",
        "\n",
        "                story.nodes[node_id] = node\n",
        "\n",
        "            return story\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Conversion error: {e}\")\n",
        "            return None\n",
        "\n",
        "# Generate story with choice of method\n",
        "story_manager = StoryGenerationManager(model_manager.current_model)\n",
        "generated_story = story_manager.choose_generation_method(story_config, final_story_concept)\n",
        "\n",
        "if generated_story:\n",
        "    print(f\"\\nüéâ Story '{generated_story.title}' ready!\")\n",
        "    print(\"üöÄ Proceeding to visualization...\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Story generation failed\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9BIm5ltK_53z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå≥ Step 8: Visualize Story Structure\n",
        "\n",
        "This step creates an interactive visualization of your story's branching structure and displays detailed information about each node. Review the story flow to ensure all paths make sense and all endings are reachable before proceeding to image generation.\n"
      ],
      "metadata": {
        "id": "IyFKjXIL_8NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class StoryVisualizer:\n",
        "    def __init__(self):\n",
        "        self.fig = None\n",
        "        self.ax = None\n",
        "\n",
        "    def visualize_story_tree(self, story: Story):\n",
        "        \"\"\"Create an interactive visualization of the story structure.\"\"\"\n",
        "        if not story:\n",
        "            print(\"‚ùå No story to visualize\")\n",
        "            return False\n",
        "\n",
        "        print(\"üå≥ Creating story tree visualization...\")\n",
        "\n",
        "        # Create directed graph\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add nodes\n",
        "        for node_id, node in story.nodes.items():\n",
        "            G.add_node(node_id,\n",
        "                      title=node.title,\n",
        "                      is_end=node.isEnd,\n",
        "                      choices=len(node.choices))\n",
        "\n",
        "        # Add edges (choices)\n",
        "        for node_id, node in story.nodes.items():\n",
        "            for choice in node.choices:\n",
        "                if choice.nextNodeId in story.nodes:\n",
        "                    G.add_edge(node_id, choice.nextNodeId,\n",
        "                              choice_text=choice.text[:30] + \"...\" if len(choice.text) > 30 else choice.text)\n",
        "\n",
        "        # Create layout\n",
        "        try:\n",
        "            pos = nx.spring_layout(G, k=3, iterations=50)\n",
        "        except:\n",
        "            pos = nx.random_layout(G)\n",
        "\n",
        "        # Create figure\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        # Draw nodes with different colors\n",
        "        start_nodes = [story.startNodeId]\n",
        "        end_nodes = [node_id for node_id, node in story.nodes.items() if node.isEnd]\n",
        "        regular_nodes = [node_id for node_id in story.nodes.keys()\n",
        "                        if node_id not in start_nodes and node_id not in end_nodes]\n",
        "\n",
        "        # Draw different node types\n",
        "        if start_nodes:\n",
        "            nx.draw_networkx_nodes(G, pos, nodelist=start_nodes,\n",
        "                                 node_color='lightgreen', node_size=1500, alpha=0.8)\n",
        "\n",
        "        if end_nodes:\n",
        "            nx.draw_networkx_nodes(G, pos, nodelist=end_nodes,\n",
        "                                 node_color='lightcoral', node_size=1500, alpha=0.8)\n",
        "\n",
        "        if regular_nodes:\n",
        "            nx.draw_networkx_nodes(G, pos, nodelist=regular_nodes,\n",
        "                                 node_color='lightblue', node_size=1200, alpha=0.8)\n",
        "\n",
        "        # Draw edges\n",
        "        nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True,\n",
        "                              arrowsize=20, arrowstyle='->', alpha=0.6)\n",
        "\n",
        "        # Add labels\n",
        "        labels = {node_id: node_id for node_id in story.nodes.keys()}\n",
        "        nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')\n",
        "\n",
        "        # Add title and legend\n",
        "        plt.title(f\"Story Structure: {story.title}\", fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "        # Create legend\n",
        "        legend_elements = [\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen',\n",
        "                      markersize=10, label='Start Node'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue',\n",
        "                      markersize=10, label='Story Node'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral',\n",
        "                      markersize=10, label='End Node')\n",
        "        ]\n",
        "        plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Display detailed node information\n",
        "        self.display_node_details(story)\n",
        "\n",
        "        # Fixed input handling\n",
        "        return self.get_approval()\n",
        "\n",
        "    def get_approval(self) -> bool:\n",
        "        \"\"\"Get user approval with better input handling.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"üìã STORY STRUCTURE REVIEW\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"Please review the visualization and node details above.\")\n",
        "\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                print(f\"\\nAttempt {attempt + 1}/{max_attempts}\")\n",
        "                approval = input(\"‚úÖ Approve this story structure? (y/n): \").strip().lower()\n",
        "\n",
        "                if approval in ['y', 'yes']:\n",
        "                    print(\"üéâ Story structure approved! Ready for image generation.\")\n",
        "                    return True\n",
        "                elif approval in ['n', 'no']:\n",
        "                    print(\"‚ùå Story not approved. You may want to regenerate with different parameters.\")\n",
        "                    return False\n",
        "                else:\n",
        "                    print(\"‚ùå Please enter 'y' for yes or 'n' for no\")\n",
        "                    continue\n",
        "\n",
        "            except (EOFError, KeyboardInterrupt):\n",
        "                print(\"\\n‚ö†Ô∏è Input interrupted. Defaulting to approved.\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Input error: {e}. Trying again...\")\n",
        "                continue\n",
        "\n",
        "        print(\"‚ö†Ô∏è Max attempts reached. Defaulting to approved.\")\n",
        "        return True\n",
        "\n",
        "    def display_node_details(self, story: Story):\n",
        "        \"\"\"Display detailed information about each node.\"\"\"\n",
        "        print(\"\\nüìã Detailed Node Information\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        for node_id, node in story.nodes.items():\n",
        "            status = \"üèÅ END\" if node.isEnd else f\"üîÄ {len(node.choices)} choices\"\n",
        "            start_marker = \"üöÄ START\" if node_id == story.startNodeId else \"\"\n",
        "\n",
        "            print(f\"\\n{start_marker} [{node_id}] {node.title} {status}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            # Show content preview\n",
        "            content_preview = node.content[:150] + \"...\" if len(node.content) > 150 else node.content\n",
        "            print(f\"Content: {content_preview}\")\n",
        "\n",
        "            # Show choices\n",
        "            if node.choices:\n",
        "                print(\"Choices:\")\n",
        "                for i, choice in enumerate(node.choices, 1):\n",
        "                    print(f\"  {i}. {choice.text} ‚Üí {choice.nextNodeId}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Visualize the generated story with fixed input\n",
        "story_approved = False\n",
        "if generated_story:\n",
        "    visualizer = StoryVisualizer()\n",
        "    story_approved = visualizer.visualize_story_tree(generated_story)\n",
        "else:\n",
        "    print(\"‚ùå No story available for visualization\")"
      ],
      "metadata": {
        "id": "yelu3fEt__j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üé® Step 9: Generate Images for Story Nodes"
      ],
      "metadata": {
        "id": "EsfVSIOKAJs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9A: Context-Aware Image Prompt Generation\n",
        "\n",
        "This step analyzes your complete story to extract consistent visual elements (characters, settings, atmosphere) and generates CLIP-optimized prompts for each node.\n",
        "\n",
        "The prompts maintain visual consistency across your entire story while being short enough for optimal Stable Diffusion performance.\n"
      ],
      "metadata": {
        "id": "Mif1mfiizKWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9A: Fixed Context-Aware Prompt Generation\n",
        "\n",
        "class ContextAwareImagePromptGenerator:\n",
        "    \"\"\"Generate image prompts with full story context for consistency.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        self.base_url = \"http://localhost:11434\"\n",
        "\n",
        "    def check_and_restart_ollama(self):\n",
        "        \"\"\"Check if Ollama is running and restart if needed.\"\"\"\n",
        "        print(\"üîç Checking Ollama server status...\")\n",
        "        try:\n",
        "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                print(\"‚úÖ Ollama server is running!\")\n",
        "                return True\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(\"‚ùå Ollama server not running. Starting it...\")\n",
        "        try:\n",
        "            subprocess.run([\"pkill\", \"-f\", \"ollama\"], capture_output=True)\n",
        "            time.sleep(2)\n",
        "\n",
        "            server_process = subprocess.Popen(\n",
        "                [\"ollama\", \"serve\"],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                env=dict(os.environ, OLLAMA_USE_CUDA='1')\n",
        "            )\n",
        "\n",
        "            print(\"‚è≥ Starting Ollama server...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                print(\"‚úÖ Ollama server restarted successfully!\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"‚ùå Server started but not responding properly\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to start Ollama server: {e}\")\n",
        "            return False\n",
        "\n",
        "    def analyze_full_story_context(self, story: Story, theme: str, selected_atmosphere: str) -> dict:\n",
        "        \"\"\"Analyze the complete story to extract consistent visual elements.\"\"\"\n",
        "        print(\"üîç Analyzing full story for visual consistency...\")\n",
        "        print(\"üìñ HOW STORY CONTEXT WORKS:\")\n",
        "        print(\"   1. Reading ALL story nodes to find common elements\")\n",
        "        print(\"   2. Extracting character types that appear across multiple nodes\")\n",
        "        print(\"   3. Identifying settings that repeat in the story\")\n",
        "        print(\"   4. Using YOUR selected atmosphere (not auto-detected)\")\n",
        "\n",
        "        # Collect ALL story content for analysis\n",
        "        all_content = []\n",
        "        all_titles = []\n",
        "        print(f\"\\nüìö Analyzing {len(story.nodes)} story nodes:\")\n",
        "\n",
        "        for i, (node_id, node) in enumerate(story.nodes.items(), 1):\n",
        "            all_content.append(node.content.lower())\n",
        "            all_titles.append(node.title.lower())\n",
        "            print(f\"   {i}. {node.title} - {len(node.content)} chars\")\n",
        "\n",
        "        # Combine all text for pattern detection\n",
        "        full_text = \" \".join(all_content + all_titles)\n",
        "        print(f\"\\nüîç Total text analyzed: {len(full_text)} characters\")\n",
        "\n",
        "        # Extract consistent characters across the story\n",
        "        characters = {}\n",
        "        character_patterns = {\n",
        "            'detective': ['detective', 'investigator', 'agent', 'cop'],\n",
        "            'businessman': ['businessman', 'executive', 'trader', 'wall street', 'corporate'],\n",
        "            'bartender': ['bartender', 'server', 'barkeeper', 'bar'],\n",
        "            'criminal': ['criminal', 'suspect', 'thief', 'gangster']\n",
        "        }\n",
        "\n",
        "        print(f\"\\nüë• Character Analysis:\")\n",
        "        for char_type, keywords in character_patterns.items():\n",
        "            count = sum(1 for keyword in keywords if keyword in full_text)\n",
        "            if count > 0:\n",
        "                characters[char_type] = f\"a {char_type}\"\n",
        "                print(f\"   ‚úÖ {char_type}: found {count} references\")\n",
        "\n",
        "        # Extract consistent settings across the story\n",
        "        settings = {}\n",
        "        setting_patterns = {\n",
        "            'speakeasy': ['speakeasy', 'bar', 'tavern', 'drinking'],\n",
        "            'war_room': ['war room', 'briefing', 'command', 'intel'],\n",
        "            'office': ['office', 'corporate', 'building', 'headquarters'],\n",
        "            'rooftop': ['rooftop', 'roof', 'building top']\n",
        "        }\n",
        "\n",
        "        print(f\"\\nüè¢ Setting Analysis:\")\n",
        "        for setting_type, keywords in setting_patterns.items():\n",
        "            count = sum(1 for keyword in keywords if keyword in full_text)\n",
        "            if count > 0:\n",
        "                settings[setting_type] = f\"a {setting_type.replace('_', ' ')}\"\n",
        "                print(f\"   ‚úÖ {setting_type}: found {count} references\")\n",
        "\n",
        "        context = {\n",
        "            'characters': characters,\n",
        "            'settings': settings,\n",
        "            'atmosphere': selected_atmosphere,  # Use the user's selected atmosphere!\n",
        "            'theme': theme,\n",
        "            'story_title': story.title\n",
        "        }\n",
        "\n",
        "        print(f\"\\n‚úÖ Story context extracted:\")\n",
        "        print(f\"   Characters: {list(characters.keys())}\")\n",
        "        print(f\"   Settings: {list(settings.keys())}\")\n",
        "        print(f\"   Atmosphere: {selected_atmosphere}\")\n",
        "\n",
        "        return context\n",
        "\n",
        "    def generate_context_aware_prompt(self, node: StoryNode, story_context: dict) -> str:\n",
        "        \"\"\"Generate SHORT prompt with story context - CLIP-friendly (under 77 tokens).\"\"\"\n",
        "\n",
        "        # Create a focused prompt that tells LLM to be concise\n",
        "        context_prompt = f\"\"\"Create a SHORT visual prompt (under 40 words) for this story scene.\n",
        "\n",
        "STORY CONTEXT (use for consistency):\n",
        "Characters: {', '.join(story_context['characters'].keys())}\n",
        "Settings: {', '.join(story_context['settings'].keys())}\n",
        "Atmosphere: {story_context['atmosphere']}\n",
        "\n",
        "CURRENT SCENE:\n",
        "Title: {node.title}\n",
        "Content: {node.content[:300]}...\n",
        "\n",
        "Create a concise visual description focusing on:\n",
        "1. Main character (if any from the context list)\n",
        "2. Setting (if any from the context list)\n",
        "3. Key visual moment\n",
        "4. Atmosphere\n",
        "\n",
        "Keep it under 40 words. No style terms - just the visual scene.\n",
        "\n",
        "Visual description:\"\"\"\n",
        "\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            if attempt > 0:\n",
        "                print(f\"üîÑ Attempt {attempt + 1}/{max_attempts}...\")\n",
        "\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    f\"{self.base_url}/api/generate\",\n",
        "                    json={\n",
        "                        \"model\": self.model_name,\n",
        "                        \"prompt\": context_prompt,\n",
        "                        \"stream\": False,\n",
        "                        \"options\": {\n",
        "                            \"temperature\": 0.7,\n",
        "                            \"top_p\": 0.9,\n",
        "                            \"num_ctx\": 4096,\n",
        "                            \"num_predict\": 60  # Limit response length\n",
        "                        }\n",
        "                    },\n",
        "                    timeout=120\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    generated_prompt = result.get('response', '').strip()\n",
        "\n",
        "                    if generated_prompt and len(generated_prompt) > 10:\n",
        "                        # Clean up response - remove any prefixes\n",
        "                        generated_prompt = generated_prompt.replace('\"', '').strip()\n",
        "\n",
        "                        # Remove common prefixes\n",
        "                        prefixes_to_remove = [\n",
        "                            'visual description:', 'scene:', 'here is', 'the scene shows',\n",
        "                            'visual prompt:', 'description:', 'scene title:', '**', '*'\n",
        "                        ]\n",
        "\n",
        "                        for prefix in prefixes_to_remove:\n",
        "                            if generated_prompt.lower().startswith(prefix):\n",
        "                                generated_prompt = generated_prompt[len(prefix):].strip()\n",
        "\n",
        "                        # Take first sentence if multiple\n",
        "                        sentences = generated_prompt.split('.')\n",
        "                        if sentences:\n",
        "                            generated_prompt = sentences[0].strip()\n",
        "\n",
        "                        # Ensure it's not too long (rough token estimate: 1 token ‚âà 4 chars)\n",
        "                        if len(generated_prompt) > 200:  # ~50 tokens\n",
        "                            words = generated_prompt.split()\n",
        "                            generated_prompt = ' '.join(words[:35])  # Limit to ~35 words\n",
        "\n",
        "                        return generated_prompt\n",
        "                else:\n",
        "                    print(f\"‚ùå Generation failed: HTTP {response.status_code}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def create_context_aware_fallback(self, node: StoryNode, story_context: dict) -> str:\n",
        "        \"\"\"Create SHORT fallback prompts using story context.\"\"\"\n",
        "        title = node.title.lower()\n",
        "        content = node.content.lower()\n",
        "\n",
        "        # Find matching character (short description)\n",
        "        character = \"a figure\"\n",
        "        for char_name in story_context['characters'].keys():\n",
        "            if char_name in content or char_name in title:\n",
        "                character = f\"a {char_name}\"\n",
        "                break\n",
        "\n",
        "        # Find matching setting (short description)\n",
        "        setting = \"indoors\"\n",
        "        for setting_name in story_context['settings'].keys():\n",
        "            if setting_name.replace('_', ' ') in content or setting_name.replace('_', ' ') in title:\n",
        "                setting = f\"in a {setting_name.replace('_', ' ')}\"\n",
        "                break\n",
        "\n",
        "        # Create short scene description\n",
        "        return f\"{character} {setting}\"\n",
        "\n",
        "# Configure style guide with SHORT options\n",
        "print(\"üé® Style Guide Configuration\")\n",
        "print(\"=\" * 50)\n",
        "print(\"Choose a style for consistent image generation:\")\n",
        "print()\n",
        "print(\"1. Film Noir (dramatic shadows, high contrast)\")\n",
        "print(\"2. Modern Cinematic (professional, sleek)\")\n",
        "print(\"3. Photorealistic (camera-like, realistic)\")\n",
        "print(\"4. Digital Art (concept art style)\")\n",
        "print(\"5. Custom (enter your own)\")\n",
        "\n",
        "style_choice = input(\"Choose style (1-5, default: 1): \").strip()\n",
        "\n",
        "# Create SHORT style guides (CLIP-friendly)\n",
        "if style_choice == \"2\":\n",
        "    style_guide = \"cinematic, professional lighting, sharp focus\"\n",
        "    atmosphere = \"modern cinematic atmosphere with professional aesthetics\"\n",
        "elif style_choice == \"3\":\n",
        "    style_guide = \"photorealistic, professional photography, dramatic lighting\"\n",
        "    atmosphere = \"realistic photographic atmosphere\"\n",
        "elif style_choice == \"4\":\n",
        "    style_guide = \"digital art, concept art style, detailed\"\n",
        "    atmosphere = \"artistic digital atmosphere\"\n",
        "elif style_choice == \"5\":\n",
        "    custom_style = input(\"Enter your SHORT style guide (under 10 words): \").strip()\n",
        "    style_guide = custom_style if custom_style else \"cinematic, dramatic lighting\"\n",
        "    atmosphere = f\"{custom_style} atmosphere\"\n",
        "else:\n",
        "    style_guide = \"film noir, dramatic shadows, high contrast\"\n",
        "    atmosphere = \"film noir atmosphere with dramatic shadows\"\n",
        "\n",
        "print(f\"‚úÖ Selected style: {style_guide}\")\n",
        "print(f\"‚úÖ Atmosphere: {atmosphere}\")\n",
        "\n",
        "# Generate context-aware prompts with PROPER style\n",
        "image_prompts = {}\n",
        "if generated_story and model_manager.current_model:\n",
        "    print(f\"\\nüß† Context-Aware Prompt Generation for {len(generated_story.nodes)} nodes\")\n",
        "\n",
        "    # Setup prompt generator\n",
        "    prompt_generator = ContextAwareImagePromptGenerator(model_manager.current_model)\n",
        "\n",
        "    # Check Ollama server\n",
        "    if prompt_generator.check_and_restart_ollama():\n",
        "        # Analyze full story context with CORRECT atmosphere\n",
        "        story_context = prompt_generator.analyze_full_story_context(\n",
        "            generated_story,\n",
        "            story_config.theme,\n",
        "            atmosphere  # Pass the correct atmosphere!\n",
        "        )\n",
        "\n",
        "        print(\"üöÄ Generating context-aware prompts...\")\n",
        "\n",
        "        # Generate prompts for each node\n",
        "        failed_prompts = []\n",
        "        total_nodes = len(generated_story.nodes)\n",
        "\n",
        "        for i, (node_id, node) in enumerate(generated_story.nodes.items(), 1):\n",
        "            print(f\"\\nüìù [{i}/{total_nodes}] Processing: {node_id}\")\n",
        "            print(f\"üìñ Title: {node.title}\")\n",
        "\n",
        "            try:\n",
        "                # Generate context-aware prompt (WITHOUT style guide)\n",
        "                scene_description = prompt_generator.generate_context_aware_prompt(node, story_context)\n",
        "\n",
        "                if scene_description:\n",
        "                    # Add style guide to create final prompt\n",
        "                    final_prompt = f\"{scene_description}, {style_guide}\"\n",
        "                    image_prompts[node_id] = final_prompt\n",
        "\n",
        "                    # Show COMPLETE prompt\n",
        "                    print(f\"‚úÖ Scene: {scene_description}\")\n",
        "                    print(f\"üé® Final: {final_prompt}\")\n",
        "                    print(f\"üìè Length: {len(final_prompt)} chars (~{len(final_prompt.split())} words)\")\n",
        "                else:\n",
        "                    # Use context-aware fallback\n",
        "                    scene_description = prompt_generator.create_context_aware_fallback(node, story_context)\n",
        "                    final_prompt = f\"{scene_description}, {style_guide}\"\n",
        "                    image_prompts[node_id] = final_prompt\n",
        "                    failed_prompts.append(node_id)\n",
        "                    print(f\"‚ö†Ô∏è Fallback: {final_prompt}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {str(e)[:50]}\")\n",
        "                scene_description = prompt_generator.create_context_aware_fallback(node, story_context)\n",
        "                final_prompt = f\"{scene_description}, {style_guide}\"\n",
        "                image_prompts[node_id] = final_prompt\n",
        "                failed_prompts.append(node_id)\n",
        "\n",
        "            progress = (i / total_nodes) * 100\n",
        "            print(f\"üìä Progress: {progress:.1f}%\")\n",
        "\n",
        "        successful = len(image_prompts) - len(failed_prompts)\n",
        "        print(f\"\\n‚úÖ Context-aware prompt generation complete!\")\n",
        "        print(f\"   Story-aware prompts: {successful}/{total_nodes}\")\n",
        "        print(f\"   Fallbacks used: {len(failed_prompts)}/{total_nodes}\")\n",
        "\n",
        "        # Save prompts to file with COMPLETE prompts visible\n",
        "        prompts_file = os.path.join(drive_manager.project_folder, \"context_aware_prompts.json\")\n",
        "        prompts_data = []\n",
        "\n",
        "        print(f\"\\nüìã COMPLETE PROMPTS GENERATED:\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for node_id, prompt in image_prompts.items():\n",
        "            prompts_data.append({\n",
        "                \"node_id\": node_id,\n",
        "                \"title\": generated_story.nodes[node_id].title,\n",
        "                \"prompt\": prompt\n",
        "            })\n",
        "            print(f\"{node_id}: {prompt}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        with open(prompts_file, 'w') as f:\n",
        "            json.dump(prompts_data, f, indent=2)\n",
        "\n",
        "        print(f\"\\nüìÅ Prompts saved to: {prompts_file}\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Cannot start Ollama server for prompt generation\")\n",
        "        image_prompts = {}\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è No story available for prompt generation\")\n",
        "    image_prompts = {}\n",
        "\n",
        "print(\"\\nüéâ Prompt generation phase complete!\")\n",
        "print(\"üí° All prompts are now CLIP-friendly (under 77 tokens)\")\n"
      ],
      "metadata": {
        "id": "N0D14Gu_RlYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9B: Memory-Optimized Image Generation\n",
        "\n",
        "Generates high-quality images using Stable Diffusion XL with aggressive memory management. The system automatically handles VRAM optimization and generates images one at a time to prevent out-of-memory errors. Choose between fast generation (SDXL Turbo) or higher quality (SDXL Base) based on your hardware.\n",
        "\n",
        "**Important:** Depending on your chosen options, you may encounter out-of-memory errors. If generation fails, restart this step and choose more conservative settings.\n",
        "\n",
        "**Safe Settings:** SDXL Turbo + 512x512 pixels + Maximum memory optimization\n",
        "These settings work reliably on most systems with 6GB+ VRAM.\n"
      ],
      "metadata": {
        "id": "sS6p3U2-vVJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install image generation libraries if needed\n",
        "try:\n",
        "    from PIL import Image\n",
        "    import torch\n",
        "    from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline, DiffusionPipeline\n",
        "    import gc\n",
        "    IMAGES_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing image generation libraries...\")\n",
        "    subprocess.run([\"pip\", \"install\", \"diffusers\", \"transformers\", \"accelerate\", \"torch\", \"torchvision\", \"Pillow\"], check=True, capture_output=True)\n",
        "    from PIL import Image\n",
        "    import torch\n",
        "    from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline, DiffusionPipeline\n",
        "    import gc\n",
        "    IMAGES_AVAILABLE = True\n",
        "    print(\"‚úÖ Image libraries installed!\")\n",
        "\n",
        "@dataclass\n",
        "class ImageConfig:\n",
        "    \"\"\"Configuration for image generation.\"\"\"\n",
        "    width: int = 512\n",
        "    height: int = 512\n",
        "    model_type: str = \"turbo\"  # \"turbo\" or \"base\"\n",
        "    inference_steps: int = 4\n",
        "    guidance_scale: float = 1.0\n",
        "    memory_optimization: str = \"2\"\n",
        "\n",
        "class MemoryOptimizedImageGenerator:\n",
        "    def __init__(self):\n",
        "        self.pipeline = None\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.setup_complete = False\n",
        "        self.config = None\n",
        "\n",
        "    def aggressive_memory_cleanup(self):\n",
        "        \"\"\"Aggressively free ALL memory before image generation.\"\"\"\n",
        "        print(\"\\nüßπ AGGRESSIVE MEMORY CLEANUP\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        try:\n",
        "            # 1. Stop Ollama server completely\n",
        "            print(\"üîÑ Stopping Ollama server...\")\n",
        "            subprocess.run([\"pkill\", \"-f\", \"ollama\"], capture_output=True)\n",
        "            time.sleep(3)\n",
        "\n",
        "            # 2. Clear any existing pipeline\n",
        "            if hasattr(self, 'pipeline') and self.pipeline is not None:\n",
        "                print(\"üóëÔ∏è Clearing existing pipeline...\")\n",
        "                del self.pipeline\n",
        "                self.pipeline = None\n",
        "\n",
        "            # 3. Clear all Python variables that might hold references\n",
        "            print(\"üóëÔ∏è Clearing Python variables...\")\n",
        "            import sys\n",
        "\n",
        "            # Clear globals that might hold model references\n",
        "            globals_to_clear = []\n",
        "            for name, obj in globals().items():\n",
        "                if hasattr(obj, '__class__') and any(keyword in str(obj.__class__).lower()\n",
        "                    for keyword in ['model', 'pipeline', 'transformer', 'diffusion']):\n",
        "                    globals_to_clear.append(name)\n",
        "\n",
        "            for name in globals_to_clear:\n",
        "                if name in globals():\n",
        "                    del globals()[name]\n",
        "                    print(f\"   Cleared: {name}\")\n",
        "\n",
        "            # 4. Aggressive garbage collection (multiple passes)\n",
        "            print(\"üóëÔ∏è Running garbage collection...\")\n",
        "            for i in range(5):  # Multiple passes for thorough cleanup\n",
        "                collected = gc.collect()\n",
        "                print(f\"   Pass {i+1}: collected {collected} objects\")\n",
        "\n",
        "            # 5. Clear CUDA cache completely\n",
        "            if torch.cuda.is_available():\n",
        "                print(\"üóëÔ∏è Clearing CUDA cache...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                # Force CUDA memory cleanup\n",
        "                torch.cuda.ipc_collect()\n",
        "\n",
        "                # Show memory status\n",
        "                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "                memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "                memory_free = memory_total - memory_allocated\n",
        "\n",
        "                print(f\"üíæ GPU Memory Status:\")\n",
        "                print(f\"   Total: {memory_total:.1f}GB\")\n",
        "                print(f\"   Allocated: {memory_allocated:.1f}GB\")\n",
        "                print(f\"   Free: {memory_free:.1f}GB\")\n",
        "\n",
        "                if memory_free < 8.0:\n",
        "                    print(\"‚ö†Ô∏è Warning: Less than 8GB free - may need smaller images\")\n",
        "\n",
        "            # 6. Clear system cache if possible\n",
        "            print(\"üóëÔ∏è Clearing system cache...\")\n",
        "            try:\n",
        "                # Try to clear system cache (Linux/Mac)\n",
        "                subprocess.run([\"sync\"], capture_output=True)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            print(\"‚úÖ Aggressive memory cleanup complete!\")\n",
        "            print(f\"üé® Maximum RAM now available for image generation\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Memory cleanup warning: {e}\")\n",
        "            print(\"üîÑ Continuing with image generation...\")\n",
        "            return False\n",
        "\n",
        "    def configure_image_settings(self) -> ImageConfig:\n",
        "        \"\"\"Interactive configuration for image generation.\"\"\"\n",
        "        print(\"üé® Memory-Optimized Image Generation Configuration\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Model selection\n",
        "        print(\"ü§ñ Model Selection:\")\n",
        "        print(\"1. SDXL Turbo (Fast, 1-4 steps, ~6GB VRAM)\")\n",
        "        print(\"2. SDXL Base (Slower, 20-30 steps, ~8GB VRAM, higher quality)\")\n",
        "\n",
        "        model_choice = input(\"Choose model (1-2, default: 1): \").strip()\n",
        "        if model_choice == \"2\":\n",
        "            model_type = \"base\"\n",
        "            inference_steps = 25\n",
        "            guidance_scale = 7.5\n",
        "            print(\"‚úÖ Selected: SDXL Base (High Quality)\")\n",
        "        else:\n",
        "            model_type = \"turbo\"\n",
        "            inference_steps = 4\n",
        "            guidance_scale = 1.0\n",
        "            print(\"‚úÖ Selected: SDXL Turbo (Fast)\")\n",
        "\n",
        "        # Image size with memory warnings\n",
        "        print(\"\\nüìê Image Size (affects VRAM usage):\")\n",
        "        print(\"1. 512x512 (Fast, ~4GB VRAM)\")\n",
        "        print(\"2. 768x768 (Better quality, ~6GB VRAM)\")\n",
        "        print(\"3. 1024x1024 (Best quality, ~10GB VRAM)\")\n",
        "\n",
        "        size_choice = input(\"Choose size (1-3, default: 1): \").strip()\n",
        "        if size_choice == \"2\":\n",
        "            width, height = 768, 768\n",
        "            print(\"‚ö†Ô∏è 768x768 requires ~6GB VRAM\")\n",
        "        elif size_choice == \"3\":\n",
        "            width, height = 1024, 1024\n",
        "            print(\"‚ö†Ô∏è 1024x1024 requires ~10GB VRAM\")\n",
        "        else:\n",
        "            width, height = 512, 512\n",
        "            print(\"‚úÖ 512x512 is memory-safe\")\n",
        "\n",
        "        # Memory optimization\n",
        "        print(\"\\nüíæ Memory Optimization:\")\n",
        "        print(\"1. Maximum (safest, CPU offloading)\")\n",
        "        print(\"2. Balanced (recommended)\")\n",
        "        print(\"3. Minimal (fastest, highest VRAM usage)\")\n",
        "        memory_choice = input(\"Choose (1-3, default: 1): \").strip()\n",
        "\n",
        "        config = ImageConfig(\n",
        "            width=width,\n",
        "            height=height,\n",
        "            model_type=model_type,\n",
        "            inference_steps=inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            memory_optimization=memory_choice if memory_choice in ['1', '2', '3'] else '1'\n",
        "        )\n",
        "\n",
        "        print(f\"\\nüìã Configuration Summary:\")\n",
        "        print(f\"  Model: {config.model_type.upper()}\")\n",
        "        print(f\"  Size: {config.width}x{config.height}\")\n",
        "        print(f\"  Steps: {config.inference_steps}\")\n",
        "        print(f\"  Memory: Level {config.memory_optimization} optimization\")\n",
        "\n",
        "        return config\n",
        "\n",
        "    def setup_image_generation(self, config: ImageConfig):\n",
        "        \"\"\"Setup pipeline with maximum memory optimization.\"\"\"\n",
        "        print(f\"\\nüé® Setting up {config.model_type.upper()} pipeline with memory optimization...\")\n",
        "\n",
        "        try:\n",
        "            # Pre-setup cleanup\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Load model with memory optimization\n",
        "            if config.model_type == \"base\":\n",
        "                print(\"üì• Loading SDXL Base (this may take a few minutes)...\")\n",
        "                self.pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "                    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_safetensors=True,\n",
        "                    variant=\"fp16\" if self.device == \"cuda\" else None\n",
        "                )\n",
        "            else:  # turbo\n",
        "                print(\"üì• Loading SDXL Turbo...\")\n",
        "                self.pipeline = DiffusionPipeline.from_pretrained(\n",
        "                    \"stabilityai/sdxl-turbo\",\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_safetensors=True,\n",
        "                    variant=\"fp16\" if self.device == \"cuda\" else None\n",
        "                )\n",
        "\n",
        "            # Move to device\n",
        "            self.pipeline = self.pipeline.to(self.device)\n",
        "\n",
        "            # Apply memory optimizations based on level\n",
        "            if self.device == \"cuda\":\n",
        "                print(\"üîß Applying memory optimizations...\")\n",
        "\n",
        "                # Always enable attention slicing\n",
        "                self.pipeline.enable_attention_slicing()\n",
        "\n",
        "                if config.memory_optimization == \"1\":  # Maximum\n",
        "                    print(\"   - CPU offloading (saves ~4GB VRAM)\")\n",
        "                    self.pipeline.enable_model_cpu_offload()\n",
        "                    print(\"   - Sequential CPU offloading\")\n",
        "                    self.pipeline.enable_sequential_cpu_offload()\n",
        "                    print(\"   - Attention slicing (slice_size=1)\")\n",
        "                    self.pipeline.enable_attention_slicing(slice_size=1)\n",
        "\n",
        "                elif config.memory_optimization == \"2\":  # Balanced\n",
        "                    print(\"   - Model CPU offloading\")\n",
        "                    self.pipeline.enable_model_cpu_offload()\n",
        "                    print(\"   - Attention slicing\")\n",
        "                    self.pipeline.enable_attention_slicing()\n",
        "\n",
        "            print(\"‚úÖ Pipeline ready with memory optimizations!\")\n",
        "            self.setup_complete = True\n",
        "            self.config = config\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Setup failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def generate_single_image(self, node_id: str, prompt: str) -> str:\n",
        "        \"\"\"Generate EXACTLY ONE image for a single node with memory management.\"\"\"\n",
        "        if not self.setup_complete:\n",
        "            return \"placeholder.jpg\"\n",
        "\n",
        "        print(f\"\\nüé® Generating image for: {node_id}\")\n",
        "        print(f\"üìù Prompt: {prompt}\")\n",
        "        print(f\"üìè Prompt length: {len(prompt)} chars, ~{len(prompt.split())} words\")\n",
        "\n",
        "        try:\n",
        "            # Pre-generation cleanup\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Show memory before generation\n",
        "            if torch.cuda.is_available():\n",
        "                memory_before = torch.cuda.memory_allocated() / 1024**3\n",
        "                print(f\"üíæ Memory before: {memory_before:.1f}GB\")\n",
        "\n",
        "            # Generate EXACTLY ONE image\n",
        "            print(\"üîÑ Generating...\")\n",
        "            if self.config.model_type == \"turbo\":\n",
        "                # SDXL Turbo settings\n",
        "                result = self.pipeline(\n",
        "                    prompt=prompt,\n",
        "                    num_inference_steps=self.config.inference_steps,\n",
        "                    guidance_scale=0.0,  # Turbo uses 0.0\n",
        "                    height=self.config.height,\n",
        "                    width=self.config.width,\n",
        "                    num_images_per_prompt=1,  # EXACTLY 1 image\n",
        "                    generator=torch.Generator(device=self.device).manual_seed(42)\n",
        "                )\n",
        "            else:  # base\n",
        "                # SDXL Base settings\n",
        "                result = self.pipeline(\n",
        "                    prompt=prompt,\n",
        "                    num_inference_steps=self.config.inference_steps,\n",
        "                    guidance_scale=self.config.guidance_scale,\n",
        "                    height=self.config.height,\n",
        "                    width=self.config.width,\n",
        "                    num_images_per_prompt=1,  # EXACTLY 1 image\n",
        "                    generator=torch.Generator(device=self.device).manual_seed(42)\n",
        "                )\n",
        "\n",
        "            # Extract the single image\n",
        "            image = result.images[0]  # Get the first (and only) image\n",
        "\n",
        "            # Save image immediately\n",
        "            filename = f\"{node_id}.png\"\n",
        "            filepath = os.path.join(drive_manager.project_folder, \"images\", filename)\n",
        "            image.save(filepath, \"PNG\", quality=95, optimize=True)\n",
        "\n",
        "            # Immediate cleanup to free memory\n",
        "            del image\n",
        "            del result\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Show memory after cleanup\n",
        "            if torch.cuda.is_available():\n",
        "                memory_after = torch.cuda.memory_allocated() / 1024**3\n",
        "                print(f\"üíæ Memory after: {memory_after:.1f}GB\")\n",
        "\n",
        "            print(f\"‚úÖ Single image saved: {filename}\")\n",
        "            return filename\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError as e:\n",
        "            print(f\"‚ùå GPU Out of Memory: {e}\")\n",
        "            print(\"üí° Try smaller image size or maximum memory optimization\")\n",
        "            return \"placeholder.jpg\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Generation failed: {e}\")\n",
        "            return \"placeholder.jpg\"\n",
        "\n",
        "# Load prompts and generate images with memory optimization\n",
        "image_results = {}\n",
        "\n",
        "# First, check if we have prompts\n",
        "if 'image_prompts' in locals() and image_prompts:\n",
        "    print(f\"\\nüé® Memory-Optimized Image Generation for {len(image_prompts)} nodes\")\n",
        "    print(\"üîÑ Process: Cleanup ‚Üí Setup ‚Üí Generate ‚Üí Cleanup per image\")\n",
        "\n",
        "    # Step 1: Aggressive memory cleanup\n",
        "    generator = MemoryOptimizedImageGenerator()\n",
        "    cleanup_success = generator.aggressive_memory_cleanup()\n",
        "\n",
        "    if cleanup_success:\n",
        "        # Step 2: Configure image generation\n",
        "        image_config = generator.configure_image_settings()\n",
        "\n",
        "        # Step 3: Setup image generation pipeline\n",
        "        if generator.setup_image_generation(image_config):\n",
        "            print(f\"\\nüöÄ Generating {len(image_prompts)} images...\")\n",
        "            print(\"üí° Each image is generated individually to minimize memory usage\")\n",
        "\n",
        "            total_nodes = len(image_prompts)\n",
        "            successful_images = 0\n",
        "\n",
        "            for i, (node_id, prompt) in enumerate(image_prompts.items(), 1):\n",
        "                print(f\"\\nüì∏ [{i}/{total_nodes}] Processing: {node_id}\")\n",
        "\n",
        "                # Generate single image with memory management\n",
        "                filename = generator.generate_single_image(node_id, prompt)\n",
        "                image_results[node_id] = filename\n",
        "\n",
        "                if filename != \"placeholder.jpg\":\n",
        "                    successful_images += 1\n",
        "\n",
        "                # Show progress\n",
        "                progress = (i / total_nodes) * 100\n",
        "                print(f\"üìä Progress: {progress:.1f}% ({successful_images}/{i} successful)\")\n",
        "\n",
        "                # Brief pause between images to let memory settle\n",
        "                if i < total_nodes:\n",
        "                    time.sleep(1)\n",
        "\n",
        "            print(f\"\\nüéâ Image generation complete!\")\n",
        "            print(f\"‚úÖ Successfully generated: {successful_images}/{total_nodes} images\")\n",
        "            print(f\"üìÅ Images saved to: {drive_manager.project_folder}/images/\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå Failed to setup image generation pipeline\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Memory cleanup failed - proceeding anyway\")\n",
        "\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è No prompts available for image generation\")\n",
        "    print(\"üí° Run the prompt generation step first\")\n",
        "\n",
        "print(\"\\n‚úÖ Memory-optimized image generation phase complete!\")\n",
        "\n",
        "# Final memory cleanup\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    memory_final = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3\n",
        "    print(f\"üíæ Final free memory: {memory_final:.1f}GB\")\n"
      ],
      "metadata": {
        "id": "ZWzAIuJxALii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Step 10: Save Final Story with Proper Format\n",
        "\n",
        "Creates the final SubQuest-compatible JSON file with proper image URLs and story structure. Handles both Google Drive and local storage, providing instructions for setting up shareable image links when needed.\n"
      ],
      "metadata": {
        "id": "dRS0TNUxANtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "class StoryExporter:\n",
        "    def __init__(self, drive_manager, image_base_url: str):\n",
        "        self.drive_manager = drive_manager\n",
        "        self.image_base_url = image_base_url\n",
        "\n",
        "    def _get_image_url(self, filename: str) -> str:\n",
        "        \"\"\"Get the proper image URL based on storage type.\"\"\"\n",
        "        if filename == \"placeholder.jpg\":\n",
        "            return \"https://images.unsplash.com/photo-1518709268805-4e9042af2176?w=800\"  # Default placeholder\n",
        "\n",
        "        if self.drive_manager.use_drive:\n",
        "            # Create proper Google Drive shareable link\n",
        "            return self._create_drive_shareable_link(filename)\n",
        "        else:\n",
        "            # For local storage, use the base URL\n",
        "            return f\"{self.image_base_url.rstrip('/')}/{filename}\"\n",
        "\n",
        "    def _create_drive_shareable_link(self, filename: str) -> str:\n",
        "        \"\"\"Create a proper Google Drive shareable link.\"\"\"\n",
        "        try:\n",
        "            # For now, we'll create a placeholder that users can replace\n",
        "            # In a full implementation, you'd use Google Drive API to get actual file IDs\n",
        "\n",
        "            # Check if file exists\n",
        "            filepath = os.path.join(self.drive_manager.project_folder, \"images\", filename)\n",
        "            if os.path.exists(filepath):\n",
        "                # Create instructions for the user\n",
        "                print(f\"üìù Note: For image '{filename}', you'll need to:\")\n",
        "                print(f\"   1. Go to Google Drive: {self.drive_manager.project_folder}/images/\")\n",
        "                print(f\"   2. Right-click on {filename} ‚Üí Share ‚Üí Copy link\")\n",
        "                print(f\"   3. Replace the placeholder URL in the JSON file\")\n",
        "\n",
        "                # Return a placeholder that's easy to find and replace\n",
        "                return f\"https://drive.google.com/file/d/REPLACE_WITH_ACTUAL_FILE_ID_FOR_{filename.replace('.', '_')}/view?usp=sharing\"\n",
        "            else:\n",
        "                return \"https://images.unsplash.com/photo-1518709268805-4e9042af2176?w=800\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not process Drive link for {filename}: {e}\")\n",
        "            return \"https://images.unsplash.com/photo-1518709268805-4e9042af2176?w=800\"\n",
        "\n",
        "\n",
        "    def create_subquest_json(self, story: 'Story', image_filenames: Dict[str, str]) -> Dict:\n",
        "        \"\"\"Create JSON in exact SubQuest format matching demo_simple.json.\"\"\"\n",
        "        if not story:\n",
        "            return None\n",
        "\n",
        "        print(\"üìù Creating SubQuest-compatible JSON...\")\n",
        "\n",
        "        # Build the JSON structure exactly like demo_simple.json\n",
        "        subquest_json = {\n",
        "            \"title\": story.title,\n",
        "            \"description\": story.description,\n",
        "            \"startNodeId\": story.startNodeId,\n",
        "            \"nodes\": {}\n",
        "        }\n",
        "\n",
        "        # Convert each node to SubQuest format\n",
        "        for node_id, node in story.nodes.items():\n",
        "            # Get image URL\n",
        "            image_filename = image_filenames.get(node_id, \"placeholder.jpg\")\n",
        "            image_url = self._get_image_url(image_filename)\n",
        "\n",
        "            # Create node in exact format\n",
        "            node_json = {\n",
        "                \"id\": node.id,\n",
        "                \"title\": node.title,\n",
        "                \"content\": node.content,\n",
        "                \"imageUrl\": image_url\n",
        "            }\n",
        "\n",
        "            # Add choices if not an ending node\n",
        "            if not node.isEnd and node.choices:\n",
        "                node_json[\"choices\"] = [\n",
        "                    {\n",
        "                        \"id\": choice.id,\n",
        "                        \"text\": choice.text,\n",
        "                        \"nextNodeId\": choice.nextNodeId\n",
        "                    }\n",
        "                    for choice in node.choices\n",
        "                ]\n",
        "\n",
        "            # Add isEnd flag for ending nodes\n",
        "            if node.isEnd:\n",
        "                node_json[\"isEnd\"] = True\n",
        "\n",
        "            subquest_json[\"nodes\"][node_id] = node_json\n",
        "\n",
        "        return subquest_json\n",
        "\n",
        "    def save_story(self, story: 'Story', image_filenames: Dict[str, str]) -> Tuple[bool, str]:\n",
        "        \"\"\"Creates the final JSON structure and saves it.\"\"\"\n",
        "        print(\"\\nüíæ Saving the final story...\")\n",
        "\n",
        "        # Create the SubQuest JSON structure\n",
        "        subquest_json_data = self.create_subquest_json(story, image_filenames)\n",
        "\n",
        "        if not subquest_json_data:\n",
        "            print(\"‚ùå Failed to create SubQuest JSON data.\")\n",
        "            return False, \"\"\n",
        "\n",
        "        # Define the output file path\n",
        "        output_filename = f\"{story.title.replace(' ', '_').lower()}_story.json\"\n",
        "        output_filepath = os.path.join(self.drive_manager.project_folder, output_filename)\n",
        "\n",
        "        try:\n",
        "            # Save the JSON data to the file\n",
        "            with open(output_filepath, 'w') as f:\n",
        "                json.dump(subquest_json_data, f, indent=2)\n",
        "\n",
        "            print(f\"‚úÖ Story JSON saved to: {output_filepath}\")\n",
        "            return True, output_filepath\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to save story JSON: {e}\")\n",
        "            return False, \"\"\n",
        "\n",
        "\n",
        "    def display_final_results(self, success: bool, filepath: str, story: 'Story'):\n",
        "        \"\"\"Display final results and file locations.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"üéâ STORY GENERATION COMPLETE!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if success:\n",
        "            print(f\"‚úÖ Story successfully saved in SubQuest format!\")\n",
        "            print(f\"üìÅ Project folder: {self.drive_manager.project_folder}\")\n",
        "            print(f\"üìÑ Story JSON: {os.path.basename(filepath)}\")\n",
        "            print(f\"üñºÔ∏è Images folder: images/\")\n",
        "\n",
        "            # Count generated images\n",
        "            images_dir = os.path.join(self.drive_manager.project_folder, \"images\")\n",
        "            if os.path.exists(images_dir):\n",
        "                image_count = len([f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "                print(f\"üì∏ Generated images: {image_count}\")\n",
        "\n",
        "            print(f\"\\nüìä Story Statistics:\")\n",
        "            print(f\"  - Title: {story.title}\")\n",
        "            print(f\"  - Total nodes: {len(story.nodes)}\")\n",
        "            print(f\"  - Ending nodes: {len([n for n in story.nodes.values() if n.isEnd])}\")\n",
        "\n",
        "            if self.drive_manager.use_drive:\n",
        "                print(f\"\\n‚òÅÔ∏è Files saved to Google Drive\")\n",
        "                print(f\"üí° You can access them in: MyDrive/SubQuest_Stories/\")\n",
        "                print(f\"\\nüîó Image URL Setup:\")\n",
        "                print(f\"  1. Go to your Google Drive folder\")\n",
        "                print(f\"  2. For each image, right-click ‚Üí Share ‚Üí Copy link\")\n",
        "                print(f\"  3. Replace the placeholder URLs in the JSON file\")\n",
        "                print(f\"  4. Make sure images are set to 'Anyone with the link can view'\")\n",
        "            else:\n",
        "                print(f\"\\nüíª Files saved locally\")\n",
        "                print(f\"üí° Upload the JSON file to your SubQuest app\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"üöÄ Next Steps:\")\n",
        "        print(\"  1. Review the generated story JSON file\")\n",
        "        print(\"  2. Update Google Drive image links if needed\")\n",
        "        print(\"  3. Test the story in your SubQuest app\")\n",
        "        print(\"  4. Share your interactive story with users!\")\n",
        "        print(\"\\nüéâ Thank you for using the Interactive Story Generator!\")\n",
        "\n",
        "# Save the final story\n",
        "if generated_story:\n",
        "    exporter = StoryExporter(drive_manager, image_base_url)\n",
        "    success, result = exporter.save_story(generated_story, image_results)\n",
        "    exporter.display_final_results(success, result, generated_story)\n",
        "else:\n",
        "    print(\"‚ùå No story available to save\")"
      ],
      "metadata": {
        "id": "0KnOLyBmARAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}